#!/usr/bin/env python3
# Copyright 2025 Sean Mooney
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""Parse review report markdown and generate zuul_return file comments.

This script parses the markdown review report generated by the AI code review
agent and extracts issues with file locations to generate zuul_return data
structure for inline comments.

Uses mistune markdown AST parser for robust, structure-aware parsing instead
of regex-based line parsing.
"""

import argparse
import contextlib
import json
import logging
import re
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import mistune


# Configure logging for debugging and operational monitoring
LOG = logging.getLogger(__name__)


@dataclass
class ReviewIssue:
    """Represents a single issue found in the review report."""

    description: str
    severity: str
    confidence: float
    location: Optional[str] = None
    file_path: Optional[str] = None
    line_number: Optional[int] = None
    risk: Optional[str] = None
    priority: Optional[str] = None
    why_matters: Optional[str] = None
    recommendation: Optional[str] = None
    message: str = field(default="", init=False)
    level: str = field(default="", init=False)

    def __post_init__(self) -> None:
        """Initialize computed fields after dataclass initialization."""
        self.level = self._map_severity_to_level(self.severity)
        self.message = self._format_message()

    def _format_message(self) -> str:
        """Format rich message with all review details."""
        parts = [
            f"{self.description} (Severity: {self.severity}, "
            f"Confidence: {self.confidence})"
        ]

        if self.risk:
            parts.append(f"Risk: {self.risk}")
        if self.priority:
            parts.append(f"Priority: {self.priority}")
        if self.why_matters:
            parts.append(f"Why This Matters: {self.why_matters}")
        if self.recommendation:
            parts.append(f"Recommendation: {self.recommendation}")

        return " | ".join(parts)

    @staticmethod
    def _normalize_file_path(file_path: str) -> str:
        """Normalize file path to be relative to git repo root."""
        if not file_path:
            return file_path

        # Remove common zuul path prefixes
        zuul_prefixes = [
            '/home/zuul/src/review.opendev.org/',
            '/home/zuul/src/opendev.org/',
            '/home/zuul/src/',
        ]

        for prefix in zuul_prefixes:
            if file_path.startswith(prefix):
                file_path = file_path.replace(prefix, '')
                # Remove the org/project prefix for cleaner paths
                if '/' in file_path:
                    parts = file_path.split('/', 1)
                    if len(parts) > 1:
                        file_path = parts[1]
                break

        return file_path

    def _map_severity_to_level(self, severity: str) -> str:
        """Map issue severity to zuul_return level."""
        severity_lower = severity.lower()
        if severity_lower in ["critical", "high"]:
            return "error"
        if severity_lower == "warning":
            return "warning"
        if severity_lower == "suggestion":
            return "info"
        return "info"


# Helper functions for AST parsing


def extract_text_from_token(token: Dict[str, Any]) -> str:
    """Recursively extract text from a token and its children."""
    if token.get('type') == 'text':
        return token.get('raw', '')

    if token.get('type') == 'codespan':
        return token.get('raw', '')

    if token.get('type') == 'softbreak':
        return ' '

    text_parts = []
    if 'children' in token:
        for child in token['children']:
            text_parts.append(extract_text_from_token(child))

    return ''.join(text_parts)


def extract_text_from_children(children: List[Dict[str, Any]]) -> str:
    """Recursively extract text from token children."""
    text_parts = []
    for child in children:
        text_parts.append(extract_text_from_token(child))
    return ''.join(text_parts)


def get_heading_text(token: Dict[str, Any]) -> str:
    """Extract text from a heading token."""
    if token.get('type') != 'heading':
        return ''
    return extract_text_from_children(token.get('children', []))


def parse_issue_header(text: str, section: str) -> Optional[Dict[str, Any]]:
    """Parse issue header line to extract severity, confidence, description.

    Args:
        text: The issue header text (after markdown parsing)
        section: The section name (Critical Issues, High Issues, etc.)

    Returns:
        Dictionary with parsed issue data, or None if not a valid issue
    """
    # Check if this looks like an issue header
    # Expected format: [severity: critical] Issue description - Confidence: 0.X
    if 'severity:' not in text:
        return None

    # Extract severity - look for [severity: ... ] pattern
    severity_match = re.search(r'\[severity:\s*([^\]]+)\]', text)
    if not severity_match:
        return None
    severity = severity_match.group(1).strip()

    # Extract confidence (optional, default to 0.5 if not present)
    confidence = 0.5
    confidence_match = re.search(r'Confidence:\s*([0-9.]+)', text)
    if confidence_match:
        with contextlib.suppress(ValueError):
            confidence = float(confidence_match.group(1))

    # Extract description (text between markers)
    description = text
    description = re.sub(r'\[severity:[^\]]+\]', '', description)
    description = re.sub(r'Confidence:\s*[0-9.]+', '', description)
    description = description.strip().lstrip('-* ').strip()

    return {
        'severity': severity,
        'confidence': confidence,
        'description': description,
        'section': section,
    }


def parse_detail_field(issue: Dict[str, Any], text: str) -> None:
    """Parse a detail field and update the issue dict.

    Args:
        issue: The issue dict to update
        text: The field text from a list item (after markdown parsing)
    """
    # Note: After mistune parsing, **Field**: becomes just "Field: "
    # Look for field markers (plain text format, not markdown)

    if text.startswith('Location:'):
        location = text.replace('Location:', '', 1).strip().strip('`')
        issue['location'] = location

        # Parse file path and line number
        if ':' in location:
            parts = location.rsplit(':', 1)
            issue['file_path'] = parts[0]

            # Handle line ranges (e.g., 135-144)
            line_info = parts[1]
            if '-' in line_info and line_info[0].isdigit():
                with contextlib.suppress(ValueError):
                    issue['line_number'] = int(line_info.split('-')[0])
            else:
                with contextlib.suppress(ValueError):
                    issue['line_number'] = int(line_info)

    elif text.startswith('Risk:'):
        issue['risk'] = text.replace('Risk:', '', 1).strip()

    elif text.startswith('Remediation Priority:'):
        issue['priority'] = text.replace('Remediation Priority:', '', 1).strip()

    elif text.startswith('Why This Matters:'):
        issue['why_matters'] = text.replace('Why This Matters:', '', 1).strip()

    elif text.startswith('Recommendation:'):
        issue['recommendation'] = text.replace('Recommendation:', '', 1).strip()

    elif text.startswith('Impact:'):
        # Warnings and some suggestions use "Impact" instead of other fields
        pass

    elif text.startswith('Suggestion:') or text.startswith('Benefit:'):
        # Suggestions use Benefit instead of Why This Matters
        pass


def parse_issue_from_list_items(
    list_items: List[Dict[str, Any]], start_idx: int, section: str
) -> Tuple[Optional[Dict[str, Any]], int]:
    """Parse a single issue from a sequence of list items.

    Args:
        list_items: List of list_item tokens
        start_idx: Index of the first item to process
        section: Current section name

    Returns:
        Tuple of (issue_dict, next_idx) or (None, next_idx)
    """
    if start_idx >= len(list_items):
        return None, start_idx

    # First item should be the issue header
    first_item = list_items[start_idx]
    first_text = extract_text_from_children(first_item.get('children', []))

    issue = parse_issue_header(first_text, section)
    if not issue:
        return None, start_idx + 1

    # Collect detail items
    idx = start_idx + 1
    while idx < len(list_items):
        item = list_items[idx]
        item_text = extract_text_from_children(item.get('children', []))

        # Check if it's a new issue (shouldn't happen in proper list structure)
        if '[severity:' in item_text and 'Confidence:' in item_text:
            break

        # Parse detail field
        parse_detail_field(issue, item_text)
        idx += 1

    return issue, idx


def parse_review_report(review_file: Path) -> Dict[str, Any]:
    """Parse the review report using mistune AST parser.

    Args:
        review_file: Path to the review-report.md file

    Returns:
        Dictionary in zuul_return format with file_comments
    """
    if not review_file.exists():
        LOG.error("Review file not found: %s", review_file)
        return {"zuul": {"file_comments": {}}}

    LOG.debug("Loading review file: %s", review_file)
    content = review_file.read_text()

    # Parse markdown to AST using mistune
    try:
        LOG.debug("Parsing markdown to AST")
        markdown = mistune.create_markdown(renderer=None)
        ast = markdown(content)
    except Exception:
        LOG.exception("Failed to parse markdown")
        return {"zuul": {"file_comments": {}}}

    # Traverse AST to find sections and issues
    all_issues = []
    current_section = None
    i = 0

    while i < len(ast):
        token = ast[i]

        # Look for section headings (level 2)
        # Note: mistune stores heading level in attrs['level'], not token['level']
        heading_level = token.get('attrs', {}).get('level') if token.get('type') == 'heading' else None
        if token.get('type') == 'heading' and heading_level == 2:
            section_name = get_heading_text(token)
            if section_name in [
                'Critical Issues', 'High Issues', 'Warnings', 'Suggestions'
            ]:
                LOG.debug("Found section: %s", section_name)
                current_section = section_name

        # Look for paragraphs with severity markers (issue headers)
        elif token.get('type') == 'paragraph' and current_section:
            paragraph_text = extract_text_from_children(
                token.get('children', [])
            )

            issue = parse_issue_header(paragraph_text, current_section)
            if issue:
                # Look ahead for a list with detail items
                if i + 1 < len(ast):
                    next_token = ast[i + 1]
                    if next_token.get('type') == 'list':
                        # Parse detail items from the list
                        for list_item in next_token.get('children', []):
                            if list_item.get('type') == 'list_item':
                                item_text = extract_text_from_children(
                                    list_item.get('children', [])
                                )
                                parse_detail_field(issue, item_text)

                        # Skip the list we just processed
                        i += 1

                # Normalize file path
                if issue.get('file_path'):
                    issue['file_path'] = (
                        ReviewIssue._normalize_file_path(
                            issue['file_path']
                        )
                    )

                # Only include issues with file locations
                if issue.get('file_path'):
                    LOG.debug("Extracted issue: %s:%s (%s)",
                              issue.get('file_path'),
                              issue.get('line_number', '?'),
                              issue.get('severity'))
                    all_issues.append(issue)

        i += 1

    # Convert parsed issues to ReviewIssue objects
    LOG.debug("Converting %d issues to ReviewIssue objects", len(all_issues))
    review_issues = []
    for issue_data in all_issues:
        try:
            issue = ReviewIssue(
                description=issue_data.get('description', ''),
                severity=issue_data.get('severity', 'unknown'),
                confidence=issue_data.get('confidence', 0.0),
                location=issue_data.get('location'),
                file_path=issue_data.get('file_path'),
                line_number=issue_data.get('line_number'),
                risk=issue_data.get('risk'),
                priority=issue_data.get('priority'),
                why_matters=issue_data.get('why_matters'),
                recommendation=issue_data.get('recommendation'),
            )
            review_issues.append(issue)
        except Exception:
            LOG.exception("Failed to create ReviewIssue")
            continue

    # Group by file for zuul_return format
    file_comments = {}
    for issue in review_issues:
        if issue.file_path not in file_comments:
            file_comments[issue.file_path] = []

        comment = {
            "message": issue.message,
            "level": issue.level,
        }

        if issue.line_number:
            comment["line"] = issue.line_number

        file_comments[issue.file_path].append(comment)

    return {
        "zuul": {
            "file_comments": file_comments
        }
    }


def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Parse review report and generate zuul_return data"
    )
    parser.add_argument(
        "review_file",
        type=Path,
        help="Path to the review-report.md file"
    )
    parser.add_argument(
        "-o", "--output",
        type=Path,
        help="Output file for JSON data (default: stdout)"
    )
    parser.add_argument(
        "--summary",
        action="store_true",
        help="Print summary of extracted issues"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Enable verbose logging output"
    )

    args = parser.parse_args()

    # Configure logging based on verbosity flag
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format="%(levelname)s: %(message)s"
    )

    LOG.debug("Starting review parsing: %s", args.review_file)

    # Parse the review report
    result = parse_review_report(args.review_file)

    # Check if parsing failed
    if not result or "zuul" not in result:
        LOG.error("Failed to parse review report")
        sys.exit(1)

    # Print summary if requested
    if args.summary:
        file_comments = result["zuul"]["file_comments"]
        total_comments = sum(
            len(comments) for comments in file_comments.values()
        )
        sys.stderr.write(
            f"Extracted {total_comments} comments across "
            f"{len(file_comments)} files\n"
        )

        # Print breakdown by severity
        severity_count = {"error": 0, "warning": 0, "info": 0}
        for comments in file_comments.values():
            for comment in comments:
                level = comment.get("level", "info")
                severity_count[level] = severity_count.get(level, 0) + 1
        sys.stderr.write(
            f"Breakdown: {severity_count['error']} errors, "
            f"{severity_count['warning']} warnings, "
            f"{severity_count['info']} info\n"
        )
        # Flush stderr to ensure it's written before stdout
        sys.stderr.flush()

    # Output the result
    json_str = json.dumps(result, indent=2)
    if args.output:
        args.output.write_text(json_str)
    else:
        sys.stdout.write(json_str + "\n")
        # Flush stdout to ensure complete output
        sys.stdout.flush()


if __name__ == "__main__":
    main()
