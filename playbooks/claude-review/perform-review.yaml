---
- name: Verify prerequisite context files exist
  stat:
    path: "{{ item }}"
  register: prereq_files
  loop:
    - "{{ review_output_dir }}/zuul-context.md"
    - "{{ review_output_dir }}/commit-summary.md"
    - "{{ ansible_user_dir }}/{{ style_guide_project }}/docs/quick-rules.md"
    - "{{ ansible_user_dir }}/{{ style_guide_project }}/docs/comprehensive-guide.md"

- name: Fail if prerequisite files missing
  fail:
    msg: "Missing prerequisite file: {{ item.item }}"
  when: not item.stat.exists
  loop: "{{ prereq_files.results }}"

- name: Set style guide paths
  set_fact:
    style_guide_path: "{{ ansible_user_dir }}/{{ style_guide_project }}/docs/quick-rules.md"
    comprehensive_guide_path: "{{ ansible_user_dir }}/{{ style_guide_project }}/docs/comprehensive-guide.md"

- name: Create code review prompt with @file references and subagent invocation (no truncation)
  set_fact:
    review_prompt: >-
      Use the @agent-code-review-agent subagent to
      perform a comprehensive code review of the change.

      Read @{{ review_output_dir }}/zuul-context.md for Zuul execution context.
      Read @{{ review_output_dir }}/commit-summary.md for commit metadata and change summary.
      Read @{{ style_guide_path }} for essential OpenStack coding standards and rules.
      Read @{{ comprehensive_guide_path }} for detailed explanations and complex scenarios.

      The project under review is located at: {{ project_src_dir }}

      Generate a structured JSON review report conforming to the review-report schema
      with severity-categorized findings (critical, high, warnings, suggestions) and write it to
      {{ review_output_dir }}/review-report.json

- name: Execute code review with Claude
  include_tasks: tasks/run-claude-command.yaml
  vars:
    prompt_text: "{{ review_prompt }}"
    model_name: "{{ review_model }}"
    output_file: "{{ review_output_dir }}/review-report.json"
    command_name: "code review"
    working_dir: "{{ project_src_dir }}"

- name: Extract issue statistics from JSON report
  shell: |
    jq -r '.statistics | "Critical: \(.critical), High: \(.high), Warnings: \(.warnings), Suggestions: \(.suggestions), Total: \(.total)"' \
      "{{ review_output_dir }}/review-report.json"
  register: issue_statistics
  changed_when: false

- name: Get total issue count from JSON
  shell: |
    jq -r '.statistics.total' "{{ review_output_dir }}/review-report.json"
  register: issue_count_result
  changed_when: false

- name: Get critical + high issue count from JSON
  shell: |
    jq -r '(.statistics.critical + .statistics.high)' "{{ review_output_dir }}/review-report.json"
  register: critical_high_count
  changed_when: false

- name: Set review result facts
  set_fact:
    issue_count: "{{ issue_count_result.stdout | int }}"
    critical_high_issues: "{{ critical_high_count.stdout | int }}"
    has_issues: "{{ issue_count_result.stdout | int > 0 }}"
    has_critical_issues: "{{ critical_high_count.stdout | int > 0 }}"

- name: Display review summary
  debug:
    msg: |
      Review Summary:
      {{ issue_statistics.stdout }}
      Status: {{ 'NEEDS ATTENTION' if has_critical_issues else ('HAS SUGGESTIONS' if has_issues else 'CLEAN') }}
