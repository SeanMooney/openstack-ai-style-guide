---
# AI Code Review Role
# Performs comprehensive code review using AI agent

- name: Verify prerequisite context files exist
  ansible.builtin.stat:
    path: "{{ item }}"
  register: ai_code_review_prereq_files
  loop:
    - "{{ ai_code_review_context_output_file }}"
    - "{{ ai_code_review_commit_summary_file }}"
    - "{{ ai_code_review_style_guide_quick_rules }}"
    - "{{ ai_code_review_style_guide_comprehensive }}"

- name: Fail if prerequisite files missing
  ansible.builtin.fail:
    msg: "Missing prerequisite file: {{ item.item }}"
  when: not item.stat.exists
  loop: "{{ ai_code_review_prereq_files.results }}"

- name: Create code review prompt with @file references and subagent invocation
  ansible.builtin.set_fact:
    ai_code_review_review_prompt: >-
      Use the @agent-code-review-agent subagent to
      perform a comprehensive code review of the change.

      Read @{{ ai_code_review_context_output_file }} for Zuul execution context.
      Read @{{ ai_code_review_commit_summary_file }} for commit metadata and change summary.
      Read @{{ ai_code_review_style_guide_quick_rules }} for essential OpenStack coding standards and rules.
      Read @{{ ai_code_review_style_guide_comprehensive }} for detailed explanations and complex scenarios.

      The project under review is located at: {{ ai_code_review_project_src_dir }}

      Generate a structured JSON review report conforming to the review-report schema
      with severity-categorized findings (critical, high, warnings, suggestions) and write it to
      {{ ai_code_review_report_file }}

- name: Execute code review with Claude
  ansible.builtin.include_role:
    name: run_claude_code
  vars:
    run_claude_code_prompt_text: "{{ ai_code_review_review_prompt }}"
    run_claude_code_model_name: "{{ ai_code_review_model }}"
    run_claude_code_output_file: "{{ ai_code_review_report_file }}"
    run_claude_code_command_name: "code review"
    run_claude_code_working_dir: "{{ ai_code_review_project_src_dir }}"
    run_claude_code_claude_binary: "{{ ai_code_review_claude_binary }}"
    run_claude_code_anthropic_auth_token: "{{ ai_code_review_anthropic_auth_token }}"
    run_claude_code_anthropic_api_url: "{{ ai_code_review_anthropic_api_url }}"

- name: Extract issue statistics from JSON report
  ansible.builtin.shell:
    cmd: >-
      set -o pipefail &&
      jq -r '.statistics |
      "Critical: \(.critical), High: \(.high),
      Warnings: \(.warnings), Suggestions: \(.suggestions),
      Total: \(.total)"'
      "{{ ai_code_review_report_file }}"
    executable: /bin/bash
  register: ai_code_review_issue_statistics
  changed_when: false

- name: Get total issue count from JSON
  ansible.builtin.shell:
    cmd: >-
      set -o pipefail &&
      jq -r '.statistics.total'
      "{{ ai_code_review_report_file }}"
    executable: /bin/bash
  register: ai_code_review_issue_count_result
  changed_when: false

- name: Get critical + high issue count from JSON
  ansible.builtin.shell:
    cmd: >-
      set -o pipefail &&
      jq -r '(.statistics.critical + .statistics.high)'
      "{{ ai_code_review_report_file }}"
    executable: /bin/bash
  register: ai_code_review_critical_high_count
  changed_when: false

- name: Set review result facts
  ansible.builtin.set_fact:
    ai_code_review_issue_count: "{{ ai_code_review_issue_count_result.stdout | int }}"
    ai_code_review_critical_high_issues: "{{ ai_code_review_critical_high_count.stdout | int }}"
    ai_code_review_has_issues: "{{ ai_code_review_issue_count_result.stdout | int > 0 }}"
    ai_code_review_has_critical_issues: "{{ ai_code_review_critical_high_count.stdout | int > 0 }}"
    ai_code_review_issue_statistics: "{{ ai_code_review_issue_statistics }}"

- name: Display review summary
  ansible.builtin.debug:
    msg: |
      Review Summary:
      {{ ai_code_review_issue_statistics.stdout }}
      Status: {{ 'NEEDS ATTENTION' if ai_code_review_has_critical_issues else ('HAS SUGGESTIONS' if ai_code_review_has_issues else 'CLEAN') }}
